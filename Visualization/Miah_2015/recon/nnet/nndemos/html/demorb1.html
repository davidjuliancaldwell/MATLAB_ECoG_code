
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Radial Basis Approximation</title><meta name="generator" content="MATLAB 8.0"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2012-07-24"><meta name="DC.source" content="demorb1.m"><link rel="stylesheet" type="text/css" href="../../../matlab/helptools/private/style.css"></head><body><div class="header"><div class="left"><a href="matlab:edit demorb1">Open demorb1.m in the Editor</a></div><div class="right"><a href="matlab:echodemo demorb1">Run in the Command Window</a></div></div><div class="content"><h1>Radial Basis Approximation</h1><!--introduction--><p>This example uses the NEWRB function to create a radial basis network that approximates a function defined by a set of data points.</p><!--/introduction--><p>Define 21 inputs P and associated targets T.</p><pre class="codeinput">X = -1:.1:1;
T = [-.9602 -.5770 -.0729  .3771  .6405  .6600  .4609 <span class="keyword">...</span>
      .1336 -.2013 -.4344 -.5000 -.3930 -.1647  .0988 <span class="keyword">...</span>
      .3072  .3960  .3449  .1816 -.0312 -.2189 -.3201];
plot(X,T,<span class="string">'+'</span>);
title(<span class="string">'Training Vectors'</span>);
xlabel(<span class="string">'Input Vector P'</span>);
ylabel(<span class="string">'Target Vector T'</span>);
</pre><img vspace="5" hspace="5" src="demorb1_01.png" alt=""> <p>We would like to find a function which fits the 21 data points.  One way to do this is with a radial basis network.  A radial basis network is a network with two layers.  A hidden layer of radial basis neurons and an output layer of linear neurons.  Here is the radial basis transfer function used by the hidden layer.</p><pre class="codeinput">x = -3:.1:3;
a = radbas(x);
plot(x,a)
title(<span class="string">'Radial Basis Transfer Function'</span>);
xlabel(<span class="string">'Input p'</span>);
ylabel(<span class="string">'Output a'</span>);
</pre><img vspace="5" hspace="5" src="demorb1_02.png" alt=""> <p>The weights and biases of each neuron in the hidden layer define the position and width of a radial basis function.  Each linear output neuron forms a weighted sum of these radial basis functions.  With the correct weight and bias values for each layer, and enough hidden neurons, a radial basis network can fit any function with any desired accuracy.  This is an example of three radial basis functions (in blue) are scaled and summed to produce a function (in magenta).</p><pre class="codeinput">a2 = radbas(x-1.5);
a3 = radbas(x+2);
a4 = a + a2*1 + a3*0.5;
plot(x,a,<span class="string">'b-'</span>,x,a2,<span class="string">'b--'</span>,x,a3,<span class="string">'b--'</span>,x,a4,<span class="string">'m-'</span>)
title(<span class="string">'Weighted Sum of Radial Basis Transfer Functions'</span>);
xlabel(<span class="string">'Input p'</span>);
ylabel(<span class="string">'Output a'</span>);
</pre><img vspace="5" hspace="5" src="demorb1_03.png" alt=""> <p>The function NEWRB quickly creates a radial basis network which approximates the function defined by P and T.  In addition to the training set and targets, NEWRB takes two arguments, the sum-squared error goal and the spread constant.</p><pre class="codeinput">eg = 0.02; <span class="comment">% sum-squared error goal</span>
sc = 1;    <span class="comment">% spread constant</span>
net = newrb(X,T,eg,sc);
</pre><pre class="codeoutput">NEWRB, neurons = 0, MSE = 0.176192
NEWRB, neurons = 2, MSE = 0.160368
NEWRB, neurons = 3, MSE = 0.128338
NEWRB, neurons = 4, MSE = 0.0275185
NEWRB, neurons = 5, MSE = 0.0264878
NEWRB, neurons = 6, MSE = 0.00046188
</pre><img vspace="5" hspace="5" src="demorb1_04.png" alt=""> <p>To see how the network performs, replot the training set.  Then simulate the network response for inputs over the same range.  Finally, plot the results on the same graph.</p><pre class="codeinput">plot(X,T,<span class="string">'+'</span>);
xlabel(<span class="string">'Input'</span>);

X = -1:.01:1;
Y = net(X);

hold <span class="string">on</span>;
plot(X,Y);
hold <span class="string">off</span>;
legend({<span class="string">'Target'</span>,<span class="string">'Output'</span>})
</pre><img vspace="5" hspace="5" src="demorb1_05.png" alt=""> <p class="footer">Copyright 1992-2012 The MathWorks, Inc.<br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2012b</a><br><br>
		  MATLAB and Simulink are registered trademarks of The MathWorks, Inc.  Please see <a href="http://www.mathworks.com/trademarks">www.mathworks.com/trademarks</a> for a list of other trademarks owned by The MathWorks, Inc.  Other product or brand names are trademarks or registered trademarks of their respective owners.
      </p></div><!--
##### SOURCE BEGIN #####
%% Radial Basis Approximation
% This example uses the NEWRB function to create a radial basis network that
% approximates a function defined by a set of data points.
%
% Copyright 1992-2012 The MathWorks, Inc.
% $Revision: 1.14.2.4 $  $Date: 2012/03/27 18:07:57 $

%%
% Define 21 inputs P and associated targets T.

X = -1:.1:1;
T = [-.9602 -.5770 -.0729  .3771  .6405  .6600  .4609 ...
      .1336 -.2013 -.4344 -.5000 -.3930 -.1647  .0988 ...
      .3072  .3960  .3449  .1816 -.0312 -.2189 -.3201];
plot(X,T,'+');
title('Training Vectors');
xlabel('Input Vector P');
ylabel('Target Vector T');

%%
% We would like to find a function which fits the 21 data points.  One way to do
% this is with a radial basis network.  A radial basis network is a network with
% two layers.  A hidden layer of radial basis neurons and an output layer of
% linear neurons.  Here is the radial basis transfer function used by the hidden
% layer.

x = -3:.1:3;
a = radbas(x);
plot(x,a)
title('Radial Basis Transfer Function');
xlabel('Input p');
ylabel('Output a');

%%
% The weights and biases of each neuron in the hidden layer define the position
% and width of a radial basis function.  Each linear output neuron forms a
% weighted sum of these radial basis functions.  With the correct weight and
% bias values for each layer, and enough hidden neurons, a radial basis network
% can fit any function with any desired accuracy.  This is an example of three
% radial basis functions (in blue) are scaled and summed to produce a function
% (in magenta).

a2 = radbas(x-1.5);
a3 = radbas(x+2);
a4 = a + a2*1 + a3*0.5;
plot(x,a,'b-',x,a2,'bREPLACE_WITH_DASH_DASH',x,a3,'bREPLACE_WITH_DASH_DASH',x,a4,'m-')
title('Weighted Sum of Radial Basis Transfer Functions');
xlabel('Input p');
ylabel('Output a');

%%
% The function NEWRB quickly creates a radial basis network which approximates
% the function defined by P and T.  In addition to the training set and targets,
% NEWRB takes two arguments, the sum-squared error goal and the spread constant.

eg = 0.02; % sum-squared error goal
sc = 1;    % spread constant
net = newrb(X,T,eg,sc);

%%
% To see how the network performs, replot the training set.  Then simulate the
% network response for inputs over the same range.  Finally, plot the results on
% the same graph.

plot(X,T,'+');
xlabel('Input');

X = -1:.01:1;
Y = net(X);

hold on;
plot(X,Y);
hold off;
legend({'Target','Output'})


displayEndOfDemoMessage(mfilename)

##### SOURCE END #####
--></body></html>