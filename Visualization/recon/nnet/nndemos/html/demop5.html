
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>Normalized Perceptron Rule</title><meta name="generator" content="MATLAB 8.0"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2012-07-24"><meta name="DC.source" content="demop5.m"><link rel="stylesheet" type="text/css" href="../../../matlab/helptools/private/style.css"></head><body><div class="header"><div class="left"><a href="matlab:edit demop5">Open demop5.m in the Editor</a></div><div class="right"><a href="matlab:echodemo demop5">Run in the Command Window</a></div></div><div class="content"><h1>Normalized Perceptron Rule</h1><!--introduction--><p>A 2-input hard limit neuron is trained to classify 5 input vectors into two categories.  Despite the fact that one input vector is much bigger than the others, training with LEARNPN is quick.</p><!--/introduction--><p>Each of the five column vectors in X defines a 2-element input vectors, and a row vector T defines the vector's target categories.  Plot these vectors with PLOTPV.</p><pre class="codeinput">X = [ -0.5 -0.5 +0.3 -0.1 -40; <span class="keyword">...</span>
      -0.5 +0.5 -0.5 +1.0 50];
T = [1 1 0 0 1];
plotpv(X,T);
</pre><img vspace="5" hspace="5" src="demop5_01.png" alt=""> <p>Note that 4 input vectors have much smaller magnitudes than the fifth vector in the upper left of the plot.  The perceptron must properly classify the 5 input vectors in X into the two categories defined by T.</p><p>PERCEPTRON creates a new network with LEARPN learning rule, which is less sensative to large variations in input vector size than LEARNP (the default).</p><p>The network is then configured with the input and target data which results in initial values for its weights and bias. (Configuration is normally not necessary, as it is done automatically by ADAPT and TRAIN.)</p><pre class="codeinput">net = perceptron(<span class="string">'hardlim'</span>,<span class="string">'learnpn'</span>);
net = configure(net,X,T);
</pre><p>Add the neuron's initial attempt at classification to the plot.</p><p>The initial weights are set to zero, so any input gives the same output and the classification line does not even appear on the plot.   Fear not... we are going to train it!</p><pre class="codeinput">hold <span class="string">on</span>
linehandle = plotpc(net.IW{1},net.b{1});
</pre><img vspace="5" hspace="5" src="demop5_02.png" alt=""> <p>ADAPT returns a new network object that performs as a better classifier, the network output, and the error.  This loop allows the network to adapt, plots the classification line, and continues until the error is zero.</p><pre class="codeinput">E = 1;
<span class="keyword">while</span> (sse(E))
   [net,Y,E] = adapt(net,X,T);
   linehandle = plotpc(net.IW{1},net.b{1},linehandle);
   drawnow;
<span class="keyword">end</span>
</pre><img vspace="5" hspace="5" src="demop5_03.png" alt=""> <p>Note that training with LEARNP took only 3 epochs, while solving the same problem with LEARNPN required 32 epochs.  Thus, LEARNPN does much better job than LEARNP when there are large variations in input vector size.</p><p>Now SIM can be used to classify any other input vector. For example, classify an input vector of [0.7; 1.2].</p><p>A plot of this new point with the original training set shows how the network performs.  To distinguish it from the training set, color it red.</p><pre class="codeinput">x = [0.7; 1.2];
y = net(x);
plotpv(x,y);
circle = findobj(gca,<span class="string">'type'</span>,<span class="string">'line'</span>);
set(circle,<span class="string">'Color'</span>,<span class="string">'red'</span>);
</pre><img vspace="5" hspace="5" src="demop5_04.png" alt=""> <p>Turn on "hold" so the previous plot is not erased.  Add the training set and the classification line to the plot.</p><pre class="codeinput">hold <span class="string">on</span>;
plotpv(X,T);
plotpc(net.IW{1},net.b{1});
hold <span class="string">off</span>;
</pre><img vspace="5" hspace="5" src="demop5_05.png" alt=""> <p>Finally, zoom into the area of interest.</p><p>The perceptron correctly classified our new point (in red) as category "zero" (represented by a circle) and not a "one" (represented by a plus). The perceptron learns properly in much shorter time in spite of the outlier (compare with the "Outlier Input Vectors" example).</p><pre class="codeinput">axis([-2 2 -2 2]);
</pre><img vspace="5" hspace="5" src="demop5_06.png" alt=""> <p class="footer">Copyright 1992-2012 The MathWorks, Inc.<br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2012b</a><br><br>
		  MATLAB and Simulink are registered trademarks of The MathWorks, Inc.  Please see <a href="http://www.mathworks.com/trademarks">www.mathworks.com/trademarks</a> for a list of other trademarks owned by The MathWorks, Inc.  Other product or brand names are trademarks or registered trademarks of their respective owners.
      </p></div><!--
##### SOURCE BEGIN #####
%% Normalized Perceptron Rule
% A 2-input hard limit neuron is trained to classify 5 input vectors into two
% categories.  Despite the fact that one input vector is much bigger than the
% others, training with LEARNPN is quick.
%
% Copyright 1992-2012 The MathWorks, Inc.
% $Revision: 1.16.2.5 $  $Date: 2012/03/27 18:07:55 $

%%
% Each of the five column vectors in X defines a 2-element input vectors, and a
% row vector T defines the vector's target categories.  Plot these vectors with
% PLOTPV.

X = [ -0.5 -0.5 +0.3 -0.1 -40; ...
      -0.5 +0.5 -0.5 +1.0 50];
T = [1 1 0 0 1];
plotpv(X,T);


%%
% Note that 4 input vectors have much smaller magnitudes than the fifth vector
% in the upper left of the plot.  The perceptron must properly classify the 5
% input vectors in X into the two categories defined by T.  
% 
% PERCEPTRON creates a new network with LEARPN learning rule, which is less
% sensative to large variations in input vector size than LEARNP (the
% default).
%
% The network is then configured with the input and target data which
% results in initial values for its weights and bias. (Configuration is
% normally not necessary, as it is done automatically by ADAPT and TRAIN.)

net = perceptron('hardlim','learnpn');
net = configure(net,X,T);

%%
% Add the neuron's initial attempt at classification to the plot.
%
% The initial weights are set to zero, so any input gives the same output and
% the classification line does not even appear on the plot.   Fear not... we are
% going to train it!

hold on
linehandle = plotpc(net.IW{1},net.b{1});

%%
% ADAPT returns a new network object that performs as a better classifier, the
% network output, and the error.  This loop allows the network to adapt,
% plots the classification line, and continues until the error is zero.

E = 1;
while (sse(E))
   [net,Y,E] = adapt(net,X,T);
   linehandle = plotpc(net.IW{1},net.b{1},linehandle);
   drawnow;
end

%%
% Note that training with LEARNP took only 3 epochs, while solving the same
% problem with LEARNPN required 32 epochs.  Thus, LEARNPN does much better job
% than LEARNP when there are large variations in input vector size.

%%
% Now SIM can be used to classify any other input vector. For example, classify
% an input vector of [0.7; 1.2].
%
% A plot of this new point with the original training set shows how the network
% performs.  To distinguish it from the training set, color it red.

x = [0.7; 1.2];
y = net(x);
plotpv(x,y);
circle = findobj(gca,'type','line');
set(circle,'Color','red');

%%
% Turn on "hold" so the previous plot is not erased.  Add the training set
% and the classification line to the plot.

hold on;
plotpv(X,T);
plotpc(net.IW{1},net.b{1});
hold off;

%%
% Finally, zoom into the area of interest.
%
% The perceptron correctly classified our new point (in red) as category "zero"
% (represented by a circle) and not a "one" (represented by a plus). The
% perceptron learns properly in much shorter time in spite of the outlier
% (compare with the "Outlier Input Vectors" example).

axis([-2 2 -2 2]);


displayEndOfDemoMessage(mfilename)

##### SOURCE END #####
--></body></html>